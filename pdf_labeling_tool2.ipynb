{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47579d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.split('==')[0].replace('-', '_'))\n",
    "        print(f\"âœ… {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"âœ… {package} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"google-cloud-vision\",\n",
    "    \"PyMuPDF\",\n",
    "    \"groq\",  # Switched from openai to groq\n",
    "    \"datasets\",\n",
    "    \"pandas\",\n",
    "    \"python-dotenv\",\n",
    "    \"pdfplumber\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a8331",
   "metadata": {},
   "source": [
    "# Environment Setup for Groq API\n",
    "\n",
    "Before running the cells below, make sure you have set up your environment variables:\n",
    "\n",
    "## 1. **Groq API Key** (Primary extraction method)\n",
    "- Get your API key from [Groq Console](https://console.groq.com/keys)\n",
    "- Add `GROQ_API_KEY=your_api_key_here` to your `.env` file\n",
    "\n",
    "## 2. **Google Cloud Vision API** (Secondary extraction method)\n",
    "Your project ID is: `105410006241`\n",
    "\n",
    "**Steps to enable:**\n",
    "1. Go to [Google Cloud Console](https://console.cloud.google.com/apis/api/vision.googleapis.com/overview?project=105410006241)\n",
    "2. Click \"ENABLE\" button to enable the Vision API\n",
    "3. Create a service account and download the JSON credentials\n",
    "4. Add `GOOGLE_APPLICATION_CREDENTIALS=path_to_your_credentials.json` to your `.env` file\n",
    "\n",
    "**Alternative: Skip Google Vision**\n",
    "If you don't want to set up Google Vision, you can modify the code to only use Groq + fallback heuristics (see cell below).\n",
    "\n",
    "## 3. **HuggingFace Token** (for uploading datasets)\n",
    "- Add `HF_TOKEN=your_hf_token_here` to your `.env` file\n",
    "\n",
    "## **Benefits of using Groq:**\n",
    "- Much faster inference speed compared to OpenAI\n",
    "- Cost-effective pricing  \n",
    "- High-quality text generation with Llama models\n",
    "- Generous rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Test with Groq only (no Google Vision required)\n",
    "import asyncio\n",
    "from dataset_generator import PDFOutlineExtractor, DatasetCreator\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "async def generate_groq_only_test():\n",
    "    \"\"\"Test with only Groq API - no Google Vision needed\"\"\"\n",
    "    \n",
    "    # Create a test directory with just a few PDFs\n",
    "    test_pdf_dir = \"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\test_pdf_subset\"\n",
    "    test_output_dir = \"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\groq_only_test\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(test_pdf_dir, exist_ok=True)\n",
    "    os.makedirs(test_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy just the first 2 PDFs for quick testing\n",
    "    original_pdf_dir = \"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\Pdf\"\n",
    "    test_files = [\"0.pdf\", \"1.pdf\"]\n",
    "    \n",
    "    for pdf_file in test_files:\n",
    "        src = os.path.join(original_pdf_dir, pdf_file)\n",
    "        dst = os.path.join(test_pdf_dir, pdf_file)\n",
    "        if os.path.exists(src) and not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"Copied {pdf_file} to test directory\")\n",
    "    \n",
    "    # Initialize extractor - use dummy path for Google credentials\n",
    "    extractor = PDFOutlineExtractor(\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        google_credentials_path=\"dummy_path\"  # Won't be used\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    creator = DatasetCreator(extractor)\n",
    "    \n",
    "    print(\"Processing test dataset with Groq only (no Google Vision)...\")\n",
    "    \n",
    "    # We'll modify this to handle Google Vision errors gracefully\n",
    "    try:\n",
    "        dataset_path = await creator.create_dataset(\n",
    "            pdf_directory=test_pdf_dir,\n",
    "            output_directory=test_output_dir\n",
    "        )\n",
    "        print(f\"Test dataset created at: {dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Some extraction methods failed (expected without Google Vision setup)\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        print(\"This is normal if Google Vision API is not set up.\")\n",
    "\n",
    "# Uncomment the line below to run this test\n",
    "# await generate_groq_only_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Open Google Cloud Console to enable Vision API\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "def setup_google_vision():\n",
    "    \"\"\"Helper function to set up Google Cloud Vision API\"\"\"\n",
    "    \n",
    "    # Your project ID from the error message\n",
    "    project_id = \"105410006241\"\n",
    "    \n",
    "    print(\"ðŸ”§ Setting up Google Cloud Vision API\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Enable the API\n",
    "    enable_url = f\"https://console.cloud.google.com/apis/api/vision.googleapis.com/overview?project={project_id}\"\n",
    "    print(f\"1. Enable Vision API:\")\n",
    "    print(f\"   Opening: {enable_url}\")\n",
    "    webbrowser.open(enable_url)\n",
    "    \n",
    "    print(\"\\n2. After enabling the API, create service account credentials:\")\n",
    "    credentials_url = f\"https://console.cloud.google.com/iam-admin/serviceaccounts?project={project_id}\"\n",
    "    print(f\"   Opening: {credentials_url}\")\n",
    "    webbrowser.open(credentials_url)\n",
    "    \n",
    "    print(\"\\nðŸ“ Next steps:\")\n",
    "    print(\"1. Click 'ENABLE' on the Vision API page\")\n",
    "    print(\"2. Create a new service account\")\n",
    "    print(\"3. Download the JSON key file\")\n",
    "    print(\"4. Add to your .env file: GOOGLE_APPLICATION_CREDENTIALS=path/to/your/key.json\")\n",
    "    print(\"\\nâœ… Once done, you can run all extraction methods!\")\n",
    "    print(\"âŒ Or run the 'Groq only' cell above to skip Google Vision\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# setup_google_vision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataset_generator import PDFOutlineExtractor, DatasetCreator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "async def generate_dataset():\n",
    "    # Initialize extractor with Groq API\n",
    "    extractor = PDFOutlineExtractor(\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),  # Changed from OPENAI_API_KEY\n",
    "        google_credentials_path=os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    creator = DatasetCreator(extractor)\n",
    "    \n",
    "    # Process PDFs and generate JSON files\n",
    "    dataset_path = await creator.create_dataset(\n",
    "        pdf_directory=\"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\Pdf\",\n",
    "        output_directory=\"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\new_json_files\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset created at: {dataset_path}\")\n",
    "\n",
    "# Run the generation - use await instead of asyncio.run() in Jupyter\n",
    "await generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a small subset of PDFs while resolving API issues\n",
    "import asyncio\n",
    "from dataset_generator import PDFOutlineExtractor, DatasetCreator\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "async def generate_test_dataset():\n",
    "    # Create a test directory with just a few PDFs\n",
    "    test_pdf_dir = \"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\test_pdf_subset\"\n",
    "    test_output_dir = \"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\test_json_files\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(test_pdf_dir, exist_ok=True)\n",
    "    os.makedirs(test_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy just the first 3 PDFs for testing\n",
    "    original_pdf_dir = \"D:\\\\VS_CODE\\\\Adobe\\\\LLM_SE_LLM_Adobe\\\\training data\\\\Pdf\"\n",
    "    test_files = [\"0.pdf\", \"1.pdf\", \"2.pdf\"]\n",
    "    \n",
    "    for pdf_file in test_files:\n",
    "        src = os.path.join(original_pdf_dir, pdf_file)\n",
    "        dst = os.path.join(test_pdf_dir, pdf_file)\n",
    "        if os.path.exists(src) and not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"Copied {pdf_file} to test directory\")\n",
    "    \n",
    "    # Initialize extractor with Groq API\n",
    "    extractor = PDFOutlineExtractor(\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),  # Changed from OPENAI_API_KEY\n",
    "        google_credentials_path=os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    creator = DatasetCreator(extractor)\n",
    "    \n",
    "    # Process PDFs and generate JSON files\n",
    "    print(\"Processing test dataset with 3 PDFs...\")\n",
    "    dataset_path = await creator.create_dataset(\n",
    "        pdf_directory=test_pdf_dir,\n",
    "        output_directory=test_output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"Test dataset created at: {dataset_path}\")\n",
    "\n",
    "# Run the test generation - this will only process 3 PDFs\n",
    "await generate_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify Groq API setup and check available models\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    from groq import Groq\n",
    "    \n",
    "    # Test if API key is set\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if api_key:\n",
    "        print(\"âœ… GROQ_API_KEY found in environment\")\n",
    "        \n",
    "        # Initialize client\n",
    "        client = Groq(api_key=api_key)\n",
    "        \n",
    "        # List available models\n",
    "        try:\n",
    "            models = client.models.list()\n",
    "            print(\"\\nðŸ“‹ Available Groq Models:\")\n",
    "            for model in models.data:\n",
    "                print(f\"  - {model.id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not list models: {e}\")\n",
    "        \n",
    "        # Test a simple API call with current model\n",
    "        print(\"\\nðŸ§ª Testing API connection...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",  # Updated to current model\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello! Just testing the connection.\"}],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(\"âœ… Groq API connection successful!\")\n",
    "        print(f\"Response: {response.choices[0].message.content}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ GROQ_API_KEY not found. Please add it to your .env file\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âŒ Groq package not installed. Run the package installation cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing Groq API: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab1b6b",
   "metadata": {},
   "source": [
    "# ðŸ” Understanding the Error Output\n",
    "\n",
    "If you ran the dataset generation and got errors, here's what they mean:\n",
    "\n",
    "## âŒ **Common Errors and Solutions:**\n",
    "\n",
    "### 1. **\"model has been decommissioned\"**\n",
    "- **Problem**: The Groq model `llama-3.1-70b-versatile` was removed\n",
    "- **Solution**: âœ… **FIXED** - Updated to use `llama-3.1-8b-instant`\n",
    "\n",
    "### 2. **\"Google Vision extraction failed\"**  \n",
    "- **Problem**: Google Cloud Vision API not enabled or network issues\n",
    "- **Solution**: Either enable Google Vision API or use \"Groq only\" mode\n",
    "\n",
    "### 3. **\"MuPDF error: No default Layer config\"**\n",
    "- **Problem**: Some PDF files have formatting issues\n",
    "- **Solution**: This is just a warning, processing continues\n",
    "\n",
    "### 4. **\"CancelledError\"**\n",
    "- **Problem**: The operation was stopped due to previous errors\n",
    "- **Solution**: Fix API issues first, then retry\n",
    "\n",
    "## âœ… **Next Steps:**\n",
    "1. Run the Groq test cell below to verify your API works\n",
    "2. Try the \"Groq only\" test with 2 PDFs (faster and simpler)\n",
    "3. Optionally set up Google Vision API later for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_upload import HuggingFaceDatasetUploader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize uploader\n",
    "uploader = HuggingFaceDatasetUploader(token=os.getenv('HF_TOKEN'))\n",
    "\n",
    "# Upload dataset\n",
    "repo_url = uploader.upload_dataset(\n",
    "    dataset_directory=\"D:\\VS_CODE\\Adobe\\LLM_SE_LLM_Adobe\\training data\\Pdf\",\n",
    "    repo_name=\"arendra/pdf-outline-extraction-dataset\",\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"Dataset uploaded: {repo_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a766fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your uploaded dataset\n",
    "dataset = load_dataset(\"arendra/pdf-outline-extraction-dataset\")\n",
    "\n",
    "# Access data\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Example: Iterate through training examples\n",
    "for example in train_data:\n",
    "    print(f\"PDF: {example['pdf_filename']}\")\n",
    "    print(f\"Title: {example['title']}\")\n",
    "    print(f\"Headings: {len(example['outline'])}\")\n",
    "    \n",
    "    for heading in example['outline']:\n",
    "        print(f\"  {heading['level']}: {heading['text']} (Page {heading['page']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33998d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results based on quality metrics\n",
    "def filter_high_quality_extractions(dataset_directory):\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    json_files = list(Path(dataset_directory).glob(\"*.json\"))\n",
    "    high_quality = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Quality criteria\n",
    "        if (len(data[\"outline\"]) >= 3 and  # At least 3 headings\n",
    "            data[\"title\"] != \"Unknown Document\" and  # Valid title\n",
    "            len(data[\"title\"]) > 10):  # Substantial title\n",
    "            high_quality.append(json_file)\n",
    "    \n",
    "    return high_quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
